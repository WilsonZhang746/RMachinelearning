data(CO2)
CO2$conc <- factor(CO2$conc)
w1b1 <- subset(CO2, Treatment=='chilled')
str(co2)
CO2
str(CO2)
head(CO2)
CO2
fit <- aov(uptake ~ (conc*Type) + Error(Plant/(conc)), w1b1)
summary(fit)
library(dplyr)
plotdata <- CO2 %>%
group_by(conc, Type) %>%
summarise(mean_conc = mean(uptake))
library(ggplot2)
ggplot(data=plotdata, aes(x=conc, y=mean_conc, group=Type, color=Type,
linetype=Type)) +
geom_point(size=2) +
geom_line(size=1) +
theme_bw() + theme(legend.position="top") +
labs(x="Concentration", y="Mean Uptake",
title="Interaction Plot for Plant Type and Concentration")
library(ggplot2)
ggplot(data=CO2, aes(x=conc, y=uptake, fill=Type)) +
geom_boxplot() +
theme_bw() + theme(legend.position="top") +
scale_fill_manual(values=c("aliceblue", "deepskyblue"))+
labs(x="Concentration", y="Uptake",
title="Chilled Quebec and Mississippi Plants")
CO2
w1b1
data(CO2)
CO2
w1b1 <- subset(CO2, Treatment=='chilled')
w1b1
fit <- aov(uptake ~ (conc*Type) + Error(Plant/(conc)), w1b1)
summary(fit)
library(dplyr)
plotdata <- CO2 %>%
group_by(conc, Type) %>%
summarise(mean_conc = mean(uptake))
plotdata
library(ggplot2)
ggplot(data=plotdata, aes(x=conc, y=mean_conc, group=Type, color=Type,
linetype=Type)) +
geom_point(size=2) +
geom_line(size=1) +
theme_bw() + theme(legend.position="top") +
labs(x="Concentration", y="Mean Uptake",
title="Interaction Plot for Plant Type and Concentration")
library(ggplot2)
ggplot(data=CO2, aes(x=conc, y=uptake, fill=Type)) +
geom_boxplot() +
theme_bw() + theme(legend.position="top") +
scale_fill_manual(values=c("aliceblue", "deepskyblue"))+
labs(x="Concentration", y="Uptake",
title="Chilled Quebec and Mississippi Plants")
library(ggplot2)
ggplot(data=plotdata, aes(x=conc, y=mean_conc, fill=Type)) +
geom_boxplot() +
theme_bw() + theme(legend.position="top") +
scale_fill_manual(values=c("aliceblue", "deepskyblue"))+
labs(x="Concentration", y="Uptake",
title="Chilled Quebec and Mississippi Plants")
ggplot(data=plotdata, aes(x=conc, y=mean_conc, group=Type, color=Type)) +
geom_boxplot() +
theme_bw() + theme(legend.position="top") +
scale_fill_manual(values=c("aliceblue", "deepskyblue"))+
labs(x="Concentration", y="Uptake",
title="Chilled Quebec and Mississippi Plants")
data(UScereal, package="MASS")
head(UScereal)
shelf <- factor(UScereal$shelf)
y <- cbind(UScereal$calories, UScereal$fat, UScereal$sugars)
y
colnames(y) <- c("calories", "fat", "sugars")
y
aggregate(y, by=list(shelf=shelf), FUN=mean)
round(cov(y), 2)
fit <- manova(y ~ shelf)
summary(fit)
summary.aov(fit)
fit.lm <- lm(response ~ trt, data=cholesterol)
data(cholesterol)
data(cholesterol, package="multcomp")
fit.lm <- lm(response ~ trt, data=cholesterol)
levels(cholesterol$trt)
fit.aov <- aov(response ~ trt, data=cholesterol)
summary(fit.aov)
fit.lm <- lm(response ~ trt, data=cholesterol)
summary(fit.lm)
contrasts(cholesterol$trt)
head(cholsetrol)
head(cholesterol)
head(cholesterol,20)
data(cholesterol, package="multcomp")
cholesterol
levels(cholesterol$trt)
fit.aov <- aov(response ~ trt, data=cholesterol)
summary(fit.aov)
fit.lm <- lm(response ~ trt, data=cholesterol)
summary(fit.lm)
contrasts(cholesterol$trt)
summary(fit.lm)
install.packages("pwr")
library(pwr)
pwr.t.test(d=.8, sig.level=.05, power=.9, type="two.sample",
alternative="two.sided")
pwr.t.test(n=20, d=.5, sig.level=.01, type="two.sample",
alternative="two.sided")
library(pwr)
pwr.t.test(d=.8, sig.level=.05, power=.9, type="two.sample",
alternative="two.sided")
pwr.t.test(n=20, d=.5, sig.level=.01, type="two.sample",
alternative="two.sided")
library(pwr)
# ANOVA
library(pwr)
pwr.anova.test(k=5, f=.25, sig.level=.05, power=.8)
library(pwr)
pwr.anova.test(k=5, f=.25, sig.level=.05, power=.8)
data(Affairs, package="AER")
install.packages("AER")
data(Affairs, package="AER")
summary(Affairs)
table(Affairs$affairs)
Affairs$ynaffair <- ifelse(Affairs$affairs > 0, 1, 0)
Affairs$ynaffair <- factor(Affairs$ynaffair,
levels=c(0,1),
labels=c("No","Yes"))
table(Affairs$ynaffair)
fit.full <- glm(ynaffair ~ gender + age + yearsmarried + children +
religiousness + education + occupation +rating,
data=Affairs,family=binomial())
summary(fit.full)
fit.reduced <- glm(ynaffair ~ age + yearsmarried + religiousness +
rating, data=Affairs, family=binomial())
summary(fit.reduced)
anova(fit.reduced, fit.full, test="Chisq")
coef(fit.reduced)
exp(coef(fit.reduced))
testdata <- data.frame(rating = c(1, 2, 3, 4, 5),
age = mean(Affairs$age),
yearsmarried = mean(Affairs$yearsmarried),
religiousness = mean(Affairs$religiousness))
testdata$prob <- predict(fit.reduced, newdata=testdata, type="response")
testdata
testdata <- data.frame(rating = mean(Affairs$rating),
age = seq(17, 57, 10),
yearsmarried = mean(Affairs$yearsmarried),
religiousness = mean(Affairs$religiousness))
testdata$prob <- predict(fit.reduced, newdata=testdata, type="response")
testdata
install.packages("robustbase")
data(epilepsy, package="robustbase")
names(epilepsy)
summary(epilepsy[6:9])
library(ggplot2)
library(patchwork)
ggplot(epilepsy, aes(x=Ysum)) +
geom_histogram(color="black", fill="white") +
labs(title="Distribution of seizures", x="Seizure Count",
y="Frequency") + theme_bw()
ggplot(epilepsy, aes(x=Trt, y=Ysum)) +
geom_boxplot() +
labs(title="Group comparisons", x="", y="") + theme_bw()
fit <- glm(Ysum ~ Base + Age + Trt, data=epilepsy, family=poisson())
summary(fit)
coef(fit)
exp(coef(fit))
data(epilepsy, package="robustbase")
names(epilepsy)
summary(epilepsy[6:9])
library(ggplot2)
ggplot(epilepsy, aes(x=Ysum)) +
geom_histogram(color="black", fill="white") +
labs(title="Distribution of seizures", x="Seizure Count",
y="Frequency") + theme_bw()
ggplot(epilepsy, aes(x=Trt, y=Ysum)) +
geom_boxplot() +
labs(title="Group comparisons", x="", y="") + theme_bw()
fit <- glm(Ysum ~ Base + Age + Trt, data=epilepsy, family=poisson())
summary(fit)
coef(fit)
exp(coef(fit))
library(psych)
install.packages("psych")
library(psych)
fa.parallel(USJudgeRatings[,-1], fa="pc", n.iter=100,
show.legend=FALSE, main="Scree plot with parallel analysis")
abline(h=1)
library(psych)
fa.parallel(USJudgeRatings[,-1], fa="pc", n.iter=100,
show.legend=FALSE, main="Scree plot with parallel analysis")
abline(h=1)
pc <- principal(USJudgeRatings[,-1], nfactors=1)
pc
library(psych)
set.seed(1234) # make results reproducible
fa.parallel(Harman23.cor$cov, n.obs=302, fa="pc", n.iter=100,
show.legend=FALSE, main="Scree plot with parallel analysis")
library(psych)
PC <- principal(Harman23.cor$cov, nfactors=2, rotate="none")
PC
pc <- principal(USJudgeRatings[,-1], nfactors=1)
pc
library(psych)
set.seed(1234)
fa.parallel(Harman23.cor$cov, n.obs=302, fa="pc", n.iter=100,
show.legend=FALSE, main="Scree plot with parallel analysis")
library(psych)
PC <- principal(Harman23.cor$cov, nfactors=2, rotate="none")
PC
rc <- principal(Harman23.cor$cov, nfactors=2, rotate="varimax")
rc
rc <- principal(Harman23.cor$cov, nfactors=2, rotate="varimax")
rc
library(psych)
pc <- principal(USJudgeRatings[,-1], nfactors=1, score=TRUE)
head(pc$scores)
pc
pc
library(psych)
rc <- principal(Harman23.cor$cov, nfactors=2, rotate="varimax")
round(unclass(rc$weights), 2)
pc <- principal(USJudgeRatings[,-1], nfactors=1, score=TRUE)
head(pc$scores)
rc <- principal(Harman23.cor$cov, nfactors=2, rotate="varimax")
round(unclass(rc$weights), 2)
library(psych)
options(digits=2)
covariances <- ability.cov$cov
covariances
correlations <- cov2cor(covariances)
correlations
set.seed(1234) # make results reproducible
fa.parallel(correlations, n.obs=112, fa="both", n.iter=100,
main="Scree plots with parallel analysis")
abline(h=c(0,1))
library(psych)
options(digits=2)
covariances <- ability.cov$cov
covariances
correlations <- cov2cor(covariances)
correlations
set.seed(1234)
fa.parallel(correlations, n.obs=112, fa="both", n.iter=100,
main="Scree plots with parallel analysis")
abline(h=c(0,1))
fa <- fa(correlations, nfactors=2, rotate="none", fm="pa")
fa
library(psych)
options(digits=2)
covariances <- ability.cov$cov
correlations <- cov2cor(covariances)
correlations
set.seed(1234) # make results reproducible
fa.parallel(correlations, n.obs=112, fa="both", n.iter=100,
main="Scree plots with parallel analysis")
abline(h=c(0,1))
fa <- fa(correlations, nfactors=2, rotate="none", fm="pa")
fa
fa.varimax <- fa(correlations, nfactors=2, rotate="varimax", fm="pa")
fa.varimax
fa.varimax$weights
fa.varimax <- fa(correlations, nfactors=2, rotate="varimax", fm="pa")
fa.varimax
fa.varimax$weights
pnorm(-1.25)
1-pnorm(1.875)
pnorm(1.875) - pnorm(-1.25)
pnorm(-1.25)
1-pnorm(1.875)
pnorm(1.875) - pnorm(-1.25)
A <- rbind(c(2,5,2),c(6,1,4))
A
t(A)
A <- diag(x=3)
A
A <- rbind(c(2,5,2),c(6,1,4))
a <- 2
a*A
A <- rbind(c(2,5,2),c(6,1,4))
A
dim(A)
B <- cbind(c(3,-1,1),c(-3,1,5))
B
dim(B)
A%*%B
A <- matrix(data=c(3,4,1,2),nrow=2,ncol=2)
A
solve(A)
A%*%solve(A)
A <- rbind(c(2,5,2),c(6,1,4))
A
t(A)
A <- diag(x=3)
A
A <- rbind(c(2,5,2),c(6,1,4))
A
a <- 2
a*A
A <- cbind(c(2,5,2),c(6,1,4))
A
B <- cbind(c(-2,3,6),c(8.1,8.2,-9.8))
B
A+B
A-B
A <- rbind(c(2,5,2),c(6,1,4))
A
dim(A)
B <- cbind(c(3,-1,1),c(-3,1,5))
B
dim(B)
A%*%B
A <- matrix(data=c(3,4,1,2),nrow=2,ncol=2)
A
solve(A)
A%*%solve(A)
pi
sqrt(2)
print(pi)
print(sqrt(2))
print(matrix(c(1, 2, 3, 4), 2, 2))
print(list("a", "b", "c"))
print("The zero occurs at", 2 * pi, "radians.")
cat("The zero occurs at", 2 * A, "radians.", "\n")
A = 3
cat("The zero occurs at", 2 * A, "radians.", "\n")
pi
print(pi)
sqrt(2)
print(sqrt(2))
print(matrix(c(1, 2, 3, 4), 2, 2))
print(list("a", "b", "c"))
print("The zero occurs at", 2 * pi, "radians.")
A = 3
cat("The zero occurs at", 2 * A, "radians.", "\n")
1:5
A <- 1:5
A
seq(from = 1, to = 5, by = 2)
B <- seq(from = 1, to = 5, by = 2)
B
rep(1, times = 5)
rep(1, 5)
1:5
A <- 1:5
A
seq(from = 1, to = 5, by = 2)
B <- seq(from = 1, to = 5, by = 2)
B
rep(1, times = 5)
rep(1, 5)
fib <- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)
fib
fib[1]
fib[2]
fib[3]
fib[1:3]         # Select elements 1 through 3
fib[4:9]         # Select elements 4 through 9
#And indexing vector neednâ€™t be a simple sequence,
fib[c(1, 2, 4, 8)]
fib[-1]
fib[-(1:3)]
fib[fib < 10]
fib[fib %% 2 == 0]
fib <- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)
fib
fib[1]
fib[2]
fib[3]
fib[1:3]
fib[4:9]
fib[c(1, 2, 4, 8)]
fib[-1]
fib[-(1:3)]
fib[fib < 10]
fib[fib %% 2 == 0]
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
data(mpg)
mpg %>%
filter(cty > 21) %>%
head(3) %>%
print()
temp1 <- filter(mpg, cty > 21)
temp2 <- head(temp1, 3)
print(temp2)
library(tidyverse)
data(mpg)
mpg
temp1 <- filter(mpg, cty > 21)
temp1
temp2 <- head(temp1, 3)
temp2
print(temp2)
data(mpg)
mpg %>%
filter(cty > 21) %>%
head(3) %>%
print()
getwd()
a <- c(1, 2, 5, 3, 6, -2, 4) # numeric
typeof(a)
b <- c("one", "two", "three") # character
typeof(b)
c <- c(TRUE, FALSE, TRUE, FALSE, TRUE, TRUE) # logical
a
b
c
x <- .Last.value
x
b <- c("one", "two", "three") # character
x <- .Last.value
x
a <- c(1, 2, 5, 3, 6, -2, 4) # numeric
typeof(a)
b <- c("one", "two", "three") # character
typeof(b)
c <- c(TRUE, FALSE, TRUE, FALSE, TRUE, TRUE) # logical
a <- c(1, 2, 5, 3, 6, -2, 4)
b <- c("one", "two", "three")
c <- c(TRUE, FALSE, TRUE, FALSE, TRUE, TRUE)
a
b
c
c
c
a
c <- c(TRUE, FALSE, TRUE, FALSE, TRUE, TRUE)
x <- .Last.value
x
setwd("d:\\RStatistics-Tutorial\\Machine-Learning-with-R-Second-Edition-master\\Chapter03")
# import the CSV file
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
# examine the structure of the wbcd data frame
str(wbcd)
setwd("d:\\RStatistics-Tutorial\\Machine-Learning-with-R-Second-Edition-master\\Chapter03")
# import the CSV file
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
# examine the structure of the wbcd data frame
str(wbcd)
# drop the id feature
wbcd <- wbcd[-1]
# table of diagnosis
table(wbcd$diagnosis)
# recode diagnosis as a factor
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
# table or proportions with more informative labels
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
# summarize three numeric features
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
# create normalization function
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# test normalization function - result should be identical
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
# normalize the wbcd data
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
# confirm that normalization worked
summary(wbcd_n$area_mean)
# create training and test data
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
# create labels for training and test data
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
## Step 3: Training a model on the data ----
# load the "class" library
library(class)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 21)
## Step 4: Evaluating model performance ----
# load the "gmodels" library
library(gmodels)
# Create the cross tabulation of predicted vs. actual
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq = FALSE)
## Step 5: Improving model performance ----
# use the scale() function to z-score standardize a data frame
wbcd_z <- as.data.frame(scale(wbcd[-1]))
# confirm that the transformation was applied correctly
summary(wbcd_z$area_mean)
# create training and test datasets
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]
# re-classify test cases
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 21)
# Create the cross tabulation of predicted vs. actual
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq = FALSE)
# try several different values of k
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=1)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=5)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=11)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=15)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=27)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
